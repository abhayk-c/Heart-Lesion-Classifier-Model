{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1VL0izNsmL1Ii1kKnSOpC_iuXsVWcqlLc",
      "authorship_tag": "ABX9TyN6hyn8B2LFS3pIgiDm/JlL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhayk-c/Heart-Lesion-Classifier-Model/blob/main/Heart_Lesion_Classifier_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Environment Setup"
      ],
      "metadata": {
        "id": "eNdkNY-J9Fk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First lets pip install all the python package dependencies we will need to build out our Classifier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y89G3wKcHlAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install kaggle\n",
        "!pip install cadica-data-set\n",
        "!pip install fastai fastcore"
      ],
      "metadata": {
        "id": "fBLsQW0_F6wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create our Data Set"
      ],
      "metadata": {
        "id": "7XGOWzft9V5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets download the cadica data set hosted on Kaggle. This data set contains labeled images of heart CT scans, labeling pictures containing arteries with lesions (potential CAD) and pictures without lesions (no CAD)."
      ],
      "metadata": {
        "id": "P-5106miR6z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir sample_data/cadica_data\n",
        "!mkdir learner\n",
        "!kaggle datasets download abhaycuram/cadica-data-set -p /content/sample_data/cadica_data --unzip"
      ],
      "metadata": {
        "id": "GaWd6w5SGe_N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have downloaded our data set, lets separately save some images from the data set for model quality analysis (analyzing inferencing and prediction quality). This will be our test data set."
      ],
      "metadata": {
        "id": "t62GVP3Nmtgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The p20/v7 file contains non lesioned images.\n",
        "# The p30/v10 file contains lesioned images.\n",
        "# We separate the video folders containing the images for these two cases\n",
        "# away as our test set to validate the inference quality of our model after fine tuning.\n",
        "!mkdir sample_data/cadica_data/test_data\n",
        "!mv /content/sample_data/cadica_data/CADICA/selectedVideos/p20/v7 /content/sample_data/cadica_data/test_data/\n",
        "!mv /content/sample_data/cadica_data/CADICA/selectedVideos/p31/v10 /content/sample_data/cadica_data/test_data/"
      ],
      "metadata": {
        "id": "-Cc0epzom5O4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we prepare our test data. All the lesioned and non lesioned images\n",
        "# we \"save\" as our test data for model quality analysis we write some scripting code\n",
        "# to generate the full paths to these images.\n",
        "\n",
        "import os\n",
        "\n",
        "def read_cadica_txt_file(path):\n",
        "  lines = []\n",
        "  f = open(path, \"r\")\n",
        "  for line in f:\n",
        "    sanitized_line = line.strip()\n",
        "    lines.append(sanitized_line)\n",
        "  f.close()\n",
        "  return lines\n",
        "\n",
        "def get_test_image_paths(vid_input_path, selected_frames_txt_path):\n",
        "  labeled_test_images = read_cadica_txt_file(selected_frames_txt_path)\n",
        "  image_files = list(filter(lambda img_file: os.path.isfile(os.path.join(vid_input_path, img_file)), os.listdir(vid_input_path)))\n",
        "  image_file_names = list(map(lambda img_file_w_ext: (os.path.splitext(img_file_w_ext))[0], image_files))\n",
        "  test_image_paths = []\n",
        "  for img_file_name in image_file_names:\n",
        "    if img_file_name in labeled_test_images:\n",
        "      test_image_paths.append(os.path.join(vid_input_path, img_file_name + \".png\"))\n",
        "  return test_image_paths\n",
        "\n",
        "# Full paths to the labeled lesioned images and non_lesioned images we will use as our test data\n",
        "# at the bottom of our notebook after model tuning.\n",
        "lesioned_test_image_paths = get_test_image_paths(\"/content/sample_data/cadica_data/test_data/v10/input/\",\n",
        "                                                 \"/content/sample_data/cadica_data/test_data/v10/p31_v10_selectedFrames.txt\")\n",
        "nonlesioned_test_image_paths = get_test_image_paths(\"/content/sample_data/cadica_data/test_data/v7/input/\",\n",
        "                                                    \"/content/sample_data/cadica_data/test_data/v7/p20_v7_selectedFrames.txt\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OvdBbGil6sOL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets read the downloaded cadica data set, index it, and prepare our training data (input images and labels). We will use the cadica-data-set package that we pip installed above to make this very easy. The package contains python objects and code that can read the cadica data set, and index it into memory and prepare the data labels to make preparing the training data very easy. This is a package I open sourced, source code can be found here:\n",
        "https://github.com/abhayk-c/cadica_data_set.git"
      ],
      "metadata": {
        "id": "HK1NaKEsbHnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cadica_data_set import CadicaDataSet\n",
        "from cadica_data_set import CadicaDataSetSamplingPolicy\n",
        "\n",
        "data_set_path = \"/content/sample_data/cadica_data/CADICA/\"\n",
        "learner_path = \"/content/learner/\"\n",
        "cadica_data_set = CadicaDataSet(data_set_path)\n",
        "cadica_data_set.load()"
      ],
      "metadata": {
        "id": "zPzzIqmJTmTu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Train and export our Model"
      ],
      "metadata": {
        "id": "1I9WwGDo97Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train and build our heart lesion classifier model. We will leverage the existing and widely popular resnet18 computer vision model and \"fine tune\" the model to learn to classify the images in our cadica data set. We leverage \"transfer learning\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Cs6OVEY82Xls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.all import *\n",
        "\n",
        "# To produce our training data (image paths) and training labels, we use our existing\n",
        "# cadica_set object that loaded our data set and directly call it's API's.\n",
        "# We wrap calling the API in named python functions to make this compatible with\n",
        "# Fast AI's DataBlock and DataLoader API.\n",
        "def get_input_image_paths(o: Any):\n",
        "    image_paths = cadica_data_set.get_training_data_image_paths(CadicaDataSetSamplingPolicy.BALANCED_SAMPLING)\n",
        "    return image_paths\n",
        "\n",
        "def get_classification_label(path: str):\n",
        "    if cadica_data_set.is_lesioned_image(path):\n",
        "        return \"lesioned_heart_scan\"\n",
        "    else:\n",
        "        return \"non_lesioned_heart_scan\"\n",
        "\n",
        "# Now let's wire up the input training/label data and create the input data used\n",
        "# for model training (Data Block and Data Loader)\n",
        "data_loaders = DataBlock(\n",
        "    blocks=(ImageBlock, CategoryBlock),\n",
        "    get_items=get_input_image_paths,\n",
        "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
        "    get_y=get_classification_label,\n",
        "    item_tfms=[RandomResizedCrop(224, min_scale=0.5)],\n",
        "    batch_tfms=aug_transforms()\n",
        ").dataloaders(learner_path, bs=32)\n",
        "data_loaders.show_batch(max_n=6)\n",
        "\n",
        "\n",
        "# Let's train our model via \"fine tuning\" and transfer learning approach, then export it.\n",
        "learn = vision_learner(data_loaders, resnet18, metrics=error_rate)\n",
        "learn.fine_tune(5)\n",
        "learn.export('heart_lesion_detector_model.pkl')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cy2enGiX23Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Evaluate Model quality"
      ],
      "metadata": {
        "id": "tTyFB5-z-R6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now run the prediction and inferencing and see how our model can accurately classify new images."
      ],
      "metadata": {
        "id": "N3lD5uUqlL60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inference = load_learner('heart_lesion_detector_model.pkl')\n",
        "lesion_prediction_hits = 0\n",
        "non_lesion_prediction_hits = 0\n",
        "prob_threshold = 0.5\n",
        "\n",
        "print(\"Expecting a lesioned label prediction\")\n",
        "for test_img in lesioned_test_image_paths:\n",
        "  label1,_,probs1 = model_inference.predict(test_img)\n",
        "  print(f\"This is a: {label1}.\")\n",
        "  print(f\"Probability it's a {label1}: {probs1[0]:.4f}\")\n",
        "  if label1 == \"lesioned_heart_scan\" and probs1[0] >= prob_threshold:\n",
        "    lesion_prediction_hits += 1\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Expecting a nonlesioned label prediction\")\n",
        "for test_img in nonlesioned_test_image_paths:\n",
        "  label1,_,probs1 = model_inference.predict(test_img)\n",
        "  print(f\"This is a: {label1}.\")\n",
        "  print(f\"Probability it's a {label1}: {probs1[0]:.4f}\")\n",
        "  if label1 == \"non_lesioned_heart_scan\" and probs1[0] >= prob_threshold:\n",
        "    non_lesion_prediction_hits += 1\n",
        "\n",
        "lesion_img_count = len(lesioned_test_image_paths)\n",
        "lesion_prediction_misses = lesion_img_count - lesion_prediction_hits\n",
        "lesion_prediction_error = (lesion_prediction_misses) / lesion_img_count\n",
        "\n",
        "nonlesion_img_count = len(nonlesioned_test_image_paths)\n",
        "nonlesion_prediction_misses = nonlesion_img_count - non_lesion_prediction_hits\n",
        "nonlesion_prediction_error = nonlesion_prediction_misses / nonlesion_img_count\n",
        "\n",
        "model_error = (lesion_prediction_misses + nonlesion_prediction_misses) / (lesion_img_count + nonlesion_img_count)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Model Quality Results:-------\")\n",
        "print(\"\\n\")\n",
        "print(f\"lesion_prediction_error: {lesion_prediction_error:.6f}\")\n",
        "print(f\"non_lesion_prediction_error: {nonlesion_prediction_error:.6f}\")\n",
        "print(f\"model_error: {model_error:.6f}\")\n"
      ],
      "metadata": {
        "id": "CaFdEza42hhw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Results and Conclusion\n",
        "\n",
        "**Observations**\n",
        "1. The ML Model performs poorly on the test data. It has an error rate of around 35.4% on the test data.\n",
        "2. Introducing data augmentations improved the model accuracy. Without data augmentations the model previously had an error rate of 48.38%\n",
        "3. The methodology we are using to assess model quality may not be very robust so there could be some false positives or false negatives.\n",
        "\n",
        "**Learnings**\n",
        "1. Leveraging a transfer learning technique on the resnet18 CNN image model to classify heart lesions may not have been a great approach due to \"out of domain\" data problem:\n",
        "  - Heart lesion angiogram pictures have a grainy low quality resolution and are black and white/grayscale. The resnet18 CNN was trained on colored images.\n",
        "  - The resnet18 CNN model was trained on the image net data set which contains real world objects based on the word net language/synset graph. These images reflect real world objects. The features in images of real world objects vs medical imaging data are fundamentally different. It might be hard for the nueral net to pick up features like arteries, veins, and organs.\n",
        "2. The cadica data set may not be enough of a representative sample of cardiac angiogram images for the neural net to learn from and \"generalize\" well to net new angiogram pictures. The resolution of these images I noticed are quite low.\n",
        "3. More advanced image manipulation and processing techniques may be needed for the ML model to learn and detect subtle features like lesioned arteries.\n",
        "\n",
        "**Improving Model Quality, things I'd do differently:**\n",
        "1. I'd curate a larger dataset of angiogram images and carefully ensure I have a goopd representative sample that generalizes well. I'd have low res images, but also high res images, and try to give the model more labeled data to learn from as this is a difficult classification task.\n",
        "2. If I wanted to continue with a tuning/transfer learning approach I would consider using a medical image ML model that has been trained on medical image data to classify things like lesions and arteries well. There are a few out there.\n",
        "3. If I wanted to continue using resnet18, Instead of finetuning (transfer learning) I would fit the model (train from scratch) and change the input layer to recognize black and white images, this could help it recognize the fine grained features in angiogram images better.\n",
        "4. More sophisticated data augmentation/transform pipeline. Introduce image transformations that can really focus on lesioned arteries and what they look like. This can help the model better isolate those features. Almost take a zoom in to zoom out approach.\n",
        "\n",
        "Ultimately a model like this could be developed in theory with more careful experimentation and iteration. Creating such a model is \"bounded\" by the data set. Getting the medical image data that we need with proper labels and quality is the limiting factor. Exploring open sourced image models trained on medical imaging data can speed up development and training cycles."
      ],
      "metadata": {
        "id": "2OyKYnRr-xrG"
      }
    }
  ]
}